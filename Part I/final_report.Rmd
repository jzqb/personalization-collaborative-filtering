---
title: "Jokes Recommendation"
subtitle: |
  Columbia University
  
  IEOR 4571 - Personalization Theory and Applications
author: "Gaurav Singh, Carolyn Silverman, Cindy Wu, Maura Fitzerald"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    theme: paper
    toc: true
---
<style type="text/css">
body {
text-align: justify;
font-size: 12pt;
max-width: 1200px;
margin-left: 100px;
margin-right: 100px;
}
body .main-container {
max-width: 1200px;
font-size: 12pt;
}
</style>
## Introduction
* Recommender systems became an important research area since the appearance of the first papers on collaborative filtering since the mid-1990s. There has been much work done both in the industry and academia on developing new approaches to recommender systems over the last decade. The interest in this area still remains high because it constitutes a problem- rich research area and because of the abundance of practical applications that help users to deal with information overload and provide personalized recommendations, content and services to them.

![Item Based Collaborative Filtering Approach.](img/cf.png)

* In this project, we build and evaluate memory based and model based recommender systems for jokes. We will be using the `recommenderlab` package in R. However, we will also demonstrate brute force coding approach to collaborative filtering.

```{r echo=TRUE, message=FALSE}
library(knitr)
read_chunk('packages_libraries.R')
read_chunk('aux_funct.R')
read_chunk('data_exploration.R')
read_chunk('neighborhood_based.R')
read_chunk('model_evaluation.R')
read_chunk('model_evaluation_plots.R')
```
### Packages and Libraries
* The following packages are used to generate the report. Since most of the plotting in the report is interactive, plotly is a good resource to harness that capability.
```{r packages, message=FALSE, warning=FALSE}
<<packages_and_libraries>>
```


## Dataset: JESTER
* 4.1 Million continuous ratings (-10.00 to +10.00) of 100 jokes from 73,421 users: collected between April 1999 - May 2003.
* 3 Data files contain anonymous ratings data from 73,421 users. We use the first data file which has data from 24,983 users.
* Data files downloaded in .zip format, when unzipped, they are in Excel (.xls) format
* Ratings are real values ranging from -10.00 to +10.00 (the value "99" corresponds to "null" = "not rated").
* One row per user
* The first column gives the number of jokes rated by that user. The next 100 columns give the ratings for jokes 01 - 100.
* The sub-matrix including only columns {5, 7, 8, 13, 15, 16, 17, 18, 19, 20} is dense. Almost all users have rated those jokes.
* JESTER is online at: [JESTER ENGINE](http://eigentaste.berkeley.edu/)

## Why recommend jokes?
* Jokes can be used to as an affiliate to get traffic on a website. Everybody likes good humor and it might be a good idea to recommend "funny" jokes to the right people to generate more clicks on an online property which drives revenue.
* The general idea of jokes (humor) can be extended to more profitable domains, like recommending books based on synposis/summary, music based on lyrics and movies based on summaries.

## Data Exploration
### Ratings Distribution
* Let's start by reading the data. Since the data is in `'.csv'` format without any headers, we read and set the headers. We also remove the first column and set all ratings with value `99` as `NA`.
```{r}
<<reading_ratings>>
```

* There are **24,983** users who have rated **100** jokes. The ratings that are given by these users lie in the range **-10.00** to **+10.00**. 
* To look at the the distribution of ratings and its density we plot the follwing graph:
```{r histogram, eval = TRUE, fig.width=12, fig.height=4, fig.align='center', message=FALSE, warning=FALSE}
<<sparsity>>
<<aux_funct_wide_to_long>>
<<histogram_ratings>>
```
* The data is not sparse at all compared to a conventional collaborative filtering dataset. Most of the ratings dataset are ~99% sparse posing a greater challenge to build statistically robust models on them.
* The distribution of the ratings appears to be multimodal, however, there is a clear distinction in frequencies on either side of **0**. A good metric to compare this would be to look at average number of ratings that was given by a user on either side of 0.
```{r sides_of_zero, eval=TRUE}
<<ratings_side_user_average>>
```
* On an average, a user rated , `r round(rat_per_user_more,0)` jokes with a rating more than 0 with an average rating of `r round(avg_rat_per_user_more,2)`. While it was `r round(rat_per_user_less,0)` jokes and `r round(avg_rat_per_user_less,2)` for less than 0 respectively!
* The average rating given by the users is `r round(mean(molten_data$value),2)`, which we will be using as our baseline model to evaluate our algorithms.

### Popular Jokes and Average Ratings Distribution
* Let us look at jokes which were rated the most. This is an indicator of popular jokes and a simple popularity based recommender can be build on top of it. But first, we will have to read in the jokes. 
* The jokes text are HTML files so we process them and store them in a data frame for our reference later.
```{r html_text_fetch}
<<parse_html>>
```
> The `joke_text` object now has text alongside the joke numbers. 
```{r}
<<popular_jokes>>
kable(top3_rated_jokes, "html", caption = "Top 3 Most Rated Jokes") %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
```
* Let's analyze few quick facts about the dataset. In terms of most rated jokes, the top 3 ones are summarized in the table. Almost 100% of the users rated the jokes 8, 13 and 17. 
* However, they are not the best of the jokes. Just for humor, let's see what the most rated joke was, Joke 8:   
<span style="color:blue">
`r joke_text$joke_text[joke_text$joke_no == 8]`
</span>
* Similarly, bottom 3 rated jokes were identified and they had ratings from ~34.5% of the users.
```{r}
kable(bottom3_rated_jokes, "html", caption = "Top 3 Least Rated Jokes") %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
```
* Jokes 71, 72 and 73 were among the least rated jokes.
* The best joke (which highest average rating) and the worst joke were identified to be Joke 50 and Joke 58 respectively. Beware, the worst joke was:   
<span style="color:blue">
`r joke_text$joke_text[joke_text$joke_no == 58]`
</span>
while the best joke was:   
<span style="color:blue">
`r joke_text$joke_text[joke_text$joke_no == 50]`
</span>
* The following table summarizes the top 3 best and worst jokes:   
```{r}
kable(best3_jokes, "html", caption = "Best 3 Jokes Based on Average Rating") %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
kable(worst3_jokes, "html", caption = "Worst 3 Jokes Based on Average Rating") %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
```

* The average ratings of the jokes had the distribution which is very intuitive, where a few jokes were exceptionally good, while most of the jokes were centered around 0.
```{r average_ratings_jokes, eval = TRUE, fig.width=12, fig.height=4, fig.align='center', message=FALSE, warning=FALSE}
<<average_ratings_jokes>>
```

### Ratings Matrix Visualization: Heat Map
```{r heat_map_visualization, eval = TRUE,fig.align='center', message=FALSE, warning=FALSE}
<<ratings_viz_heat>>
```
* This lets us visualize jokes which are highly rated. Some red vertical striped are very clearly seen, which are the jokes which are rated good by the users. 
* Also, some vertical blue stripes are also visible. A user might have tendency to give better ratings than other users, in that case a consistent color horizontal patch might help us to identify such patterns. 
* Most of the image in the right, is white implying that the items towards the end had not been rated much. This brings the sparsity to the data and it is good to know that most of the sparsity is coming from one end of data.
* Another quick way of visualization for the ratings is to identify the jokes and users with a minimum cut-off. This way, we get to know about the opinions of people on popular jokes and *active* users. In other words, the following chart gives us most relevant jokes and users.
```{r echo=TRUE, eval = TRUE, fig.align='center', message=FALSE, warning=FALSE}
<<ratings_viz_heat_segment>>
```
* Clear patterns of colored patches are observed in the figure above. We sampled the data such that minimum number of jokes rated by the user was greater than the 70th percentile of distribution of number of ratings by the user. 
* A similar cut-off for number of user ratings was applied on jokes, but with the 75th percentile filter.   
------------------   


## Recommender Algorithms
### Setting Up Data
* The total number of ratings that we have is `r nrow(molten_data)`. However, we split the data into training and test sets and let the proportion be one of the parameters to evaluate the algorithms.
* Here is a function to do the job.
```{r}
<<aux_funct_train_test_split>>
```
* We will vary the train proportion and evaluate the recommender systems that are to be developed in the following sections. We set the seed to reporduce the results in the report.


### Collaborative Filtering: Item Based

* Item-item collaborative filtering, or item-based is a form of collaborative filtering for recommender systems based on the similarity between items calculated using people's ratings of those items.   
![Item Based Collaborative Filtering Approach.](img/item.png)
* First, the system executes a model-building stage by finding the similarity between all pairs of items. This similarity function can take many forms and here we will be using Pearson, Cosine and Jaccard similarity.
* Second, the system executes a recommendation stage. It uses the most similar items to a user's already-rated items to generate a list of recommendations. Usually this calculation is a weighted sum. This form of recommendation is analogous to "people who rate item X highly, like you, also tend to rate item Y highly, and you haven't rated item Y yet, so you should try it".
* With more users than items, each item tends to have more ratings than each user, so an item's average rating usually doesn't change quickly. This leads to more stable rating distributions in the model, so the model doesn't have to be rebuilt as often. When users consume and then rate an item, that item's similar items are picked from the existing system model and added to the user's recommendations.
* The following image represents the similarity of jokes based on the ratings and pearson correlation.
```{r echo=TRUE, eval = TRUE, fig.align='center', message=FALSE, warning=FALSE}
<<item_similarity>>
```
* Firstly, we use the package function to build up our IBCF model.
```{r}
<<aux_funct_wide_to_long>>
<<item_based_collaborative_filter>>
```
* In the above code, we take 30 most similar jokes and use them to predict ratings. We are using the pearson similarity to identify the neighbors. 
* A quick plot of the heatmap of the similarity matrix will show us that only 30 jokes are being used and most of the entries will be 0.
```{r echo=TRUE, eval = TRUE, fig.align='center', message=FALSE, warning=FALSE}
<<similarity_matrix_model>>
```
* Furthermore, to test out the similarity matrix, we can see the matrix only has 30 entires per joke and rest are zero. Also, it is interesting to see which jokes are most similar to other jokes. In this case, it is joke number 98.
```{r}
<<similar_jokes_top>>
```
* Again, just to see for humor, what joke 98 is:   
<span style="color:blue">
`r joke_text$joke_text[joke_text$joke_no == 98]`
</span>   

* We can predict the ratings for the users and test the Mean Absolute Error on the testing set.
```{r}
<<predicted_ratings_item_based_algorithm>>
```
* In this case, we get a MAE to be `r round(pearson_ibcf_mae,2)`. Can we do better than this? Let us try a few other models to find out.
* We will be further evaluating the model above in the evaluation section to report the optimal setting for this algorithm.
* Additionally, we can look at top - N products recommended to user, which is most often required in a production setting.
```{r}
<<top_n_ibcf>>
```

* We now build the algorithm from the scratch from the following code and quickly measure its performance. We will not be evaluating this code through cross-validation in the further section.

```{r, echo=TRUE, eval=FALSE}
<<item_based_brute_force>>
```
* MAE on the testing test for this data turn out to be 3.66 for cosine similarity and 3.63 for pearson similarity.

### Collaborative Filtering: User Based
* The user based recommendation algorithm uses a similarity-based vector model to identify the k most similar users to an active user. 
* After the k most similar users are found, their corresponding user-item matrices are aggregated to identify the set of items to be recommended. 
* It uses the fact that people who agreed in the past are likely to agree again. * Similarity between users is decided by looking at their overlap in opinions for other items.
* The parameters that we pass in the model can be seen below:
```{r}
<<user_based_parameters>>
```
* The method arguments takes in the similarity metric that will be used to calculate user similarity. We demonstrate for cosine [default] similarity here, howerver in evaluation we can check which similarity metric works best for this data set.
* We have over 24k users in our dataset, so we are skipping visualization of the complete user-user similarity matrix, which can be done in the same way as item-item similarity.
* We build our user based collaborative filter with 25 [default] nearest neighbours as follows:

```{r eval = TRUE, echo = FALSE, warnings = FALSE, messages = FALSE}
<<source_ubcf_data>>
```

```{r eval=TRUE, echo=TRUE}
<<user_based_collaborative_filter>>
```
* Once the model is learnt, we can test out the performance on the testing set.
```{r eval = FALSE, echo= TRUE}
<<predicted_ratings_user_based_algorithm>>
```
```{r}
<<ubcf_mae>>
```

* The coverage of this model can be visualized from the following plot.
```{r}
<<ubcf_coverage_viz>>
```

* MAE for this setting is `r 3.36`, which is higher than IBCF. We will evaluate further and tune our hyper-parameters in evaluation section.
* The User Based Collaborative Filter coverage can be looked in the same way as Item Based, by looking at the top-k recommendations for the users. Here we consider what are the top 5 recommended jokes for the users.
![Item Based Collaborative Filtering Approach.](img/user.png)

* The figure above shows the filtering of users who rated the same items.

### Model Based: Frequent Pattern Mining
### Model Based: Matrix Facrtorization

## Evaluation
In this section, we evaluate and compare six different models:  

* Item-based collaborative filtering with Pearson correlation  
* Item-based collaborative filtering with cosine similarity  
* User-based collaborative filtering with Pearson correlation  
* User-based collaborative filtering with cosine similarity  
* SVD for Matrix Factorization 
* A random model (baseline)

After we determine the best similarity metrics for item-based CF and user-based CF, we tune the parameters of our chosen models using cross-validation. The parameters we need to tune are: 

* Neighborhood size k for item-based CF
* Neighborhood size nn for user-based CF
* (description) k for SVD

Finally, we evaluate the overall accuracy of the models, as functions of the following variables:  

* proportion of the dataset we train on
* number of ratings given per user in the test set

The following sections will elaborate on each of these tasks and display the results.


### Cross Validation

We start by limiting the dataset to the first 10,000 users to make training all of the models more manageable. Then, we split the ratings data into a training set and a test set based on users. For our initial evaulations, we will train on 80% of users and test our models on the other 20%.
```{r  echo = TRUE, eval=FALSE}
<<data_setup>>
```

Next, we devise an evaluation scheme on the training set to select the best models and tune parameters. We will use the evaluationScheme function in the recommenderlab package. We assign initial values (or a range of values to test) to the following parameters:   

* n_fold: Number of cross validation folds  
* items_to_keep: number of items given for test users; the rest are withheld for evauation  
* threshold: jokes with a rating higher than this threshhold will be considered a positive rating in the binary representation    
* n_recommendations: number of recommendations for the top-n lists  
* k_items: hyperparameter for item-based cf (neighborhood size)  
* nn_users: hyperparameter for user-based cf (neighborhood size)  

```{r echo = TRUE, eval=FALSE}
<<eval_scheme>>
```

Now we are ready to test our models. We start with four different collaborative filtering models, SVD (our model-based approach), and one random model to use as our baseline. For each, we will train 2 separate models: one to predict top-N lists and one to predict real rating values. 

```{r echo = TRUE, eval=FALSE}
<<compare_cf_models>>
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#loading the model data so we don't have to re-train models every time we knit html
<<load_plot_data>>
```

First, we evaluate top-N lists for each model:
```{r, fig.show='hold', out.width='50%'}
<<compare_cf_topN>>
```

Next, we compare error metrics for the ratings.
```{r}
<<compare_cf_ratings>>
```

The results indicate that user-based collaborative filtering with cosine similarity performs best on the jester dataset. For the item-based approaches, Pearson correlation outperforms cosine similarity. Let's now use cross-validation to tune the hyperparameter for each of these models.

```{r echo = TRUE, eval=FALSE}
<<ibcf_tune_k>>
```

```{r, fig.show='hold', out.width='50%'}
<<ibcf_tune_k_plots>>
```

The optimal k for item-based CF is 5 or 10 when we want to predict top-N lists. However, if we want to predict the ratings, a higher value of k is better. The plot of MAE versus neighborhood size starts to level off after k=30, so we will use k=30 as the neighborhood size for our Pearson item-based CF model when we want to predict ratings.

Next we use the same approach to tune the neighborhood size parameter for user-based collaborative filtering:
```{r echo = TRUE, eval=FALSE}
<<ubcf_tune_nn>>
```

```{r, fig.show='hold', out.width='50%'}
<<ubcf_tune_nn_plots>>
```

From the above plots, we see that the best neighborhood size for user-based CF is 250. However, nn=250 just barely outperforms nn=100, so we will choose a neighborhood size of 100 in our final model to improve running time.

Finally, let's tune the parameter k for the model-based approach...

```{r echo = TRUE, eval=FALSE}
#code for tuning k
```

```{r, fig.show='hold', out.width='50%'}
#code for plotting k
```


### Accuracy Measures

For our final chosen algorithms, let's examine how our prediction error, as measured by MAE, changes with training size. Instead of using training size explicitly, we use proportion of the data trained on. Since the total size of the limited dataset is 10,000, a portion of .2 would mean that we trained the model on 2,000 users and tested it on the other 8,000.
```{r echo = TRUE, eval=FALSE}
<<err_vs_size>>
```

```{r}
<<err_vs_size_all_plot>>
```

The plot indicates that for both models, we only need to train on 10-20% of the users before MAE stabilizes.

An additional question to consider is the following: how many ratings for a new user do we need in order to make accurate predictions? In the above analyses, we provided 15 ratings per test user, but might we get better results if we increased this number? To answer this question, let's plot prediction error (MAE) as a function of number of ratings given per user in the test set. Intuitively, this number must be less than the number of ratings supplied by the user for which we have the least information (here, it is 36). Otherwise, we would be unable to test our predictions! 

```{r echo = TRUE, eval=FALSE}
<<err_vs_numitems>>
```

```{r}
<<err_vs_numitems_plot>>
```

The plot shows that the MAE of our collaborative filtering models do not level out as the number of items given approaches 35, which implies that we need more information about a user's sense of humor (i.e. more joke ratings) to generate more accurate predictions. In order to obtain this information, we will train and test one final set of models on the of subset users for which we have complete information (that is 100 joke ratings). This subset has 2899 users. We will train on 80% of these users and test on the other 20%.

```{r echo = TRUE, eval=FALSE}
<<err_vs_numitems_complete>>
```

```{r}
<<err_vs_numitems_complete_plot>>
```

We see an uninterrupted decline in MAE until we reach 45 ratings for user-based CF and 50 ratings given for item-based CF. After these points, MAE begins to fluctuate more drastically. Therefore, we recommend that if a user wants us to predict his rating for an unseen joke, he should provide us with around 50 ratings for other jokes so that the models can better learn his sense of humor.

## Run Time Analysis

## Conclusion

## References
